{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emjOelSGCjnJ",
        "outputId": "aca88b2b-715d-4302-e4e2-0878271b4757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree: {'Outlook': {'Sunny': {'Temperature': {'Hot': 'No', 'Mild': {'Windy': {'Weak': 'No', 'Strong': 'Yes'}}}}, 'Overcast': 'Yes', 'Rainy': 'Yes'}}\n",
            "Pruned Decision Tree: {'Outlook': {'Sunny': {'Temperature': {'Hot': 'No', 'Mild': {'Windy': {'Weak': 'No', 'Strong': 'Yes'}}}}, 'Overcast': 'Yes', 'Rainy': 'Yes'}}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Helper function to calculate entropy\n",
        "def entropy(dataset):\n",
        "    # Count the occurrences of each class label in the dataset\n",
        "    class_counts = dataset.iloc[:, -1].value_counts()\n",
        "    # Calculate the entropy of the dataset using the formula\n",
        "    prob = class_counts / len(dataset)\n",
        "    return -np.sum(prob * np.log2(prob))\n",
        "\n",
        "# 2. Function to calculate information gain\n",
        "def information_gain(dataset, feature):\n",
        "    # Calculate the entropy of the original dataset\n",
        "    total_entropy = entropy(dataset)\n",
        "\n",
        "    # Get the values of the feature and their frequencies\n",
        "    feature_values = dataset[feature].value_counts()\n",
        "    weighted_entropy = 0\n",
        "\n",
        "    # For each feature value, calculate the weighted entropy of the split\n",
        "    for value, count in feature_values.items():\n",
        "        subset = dataset[dataset[feature] == value]\n",
        "        weighted_entropy += (count / len(dataset)) * entropy(subset)\n",
        "\n",
        "    # Return the information gain as the difference between the total entropy and the weighted entropy\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "# 3. Function to get the best feature to split on\n",
        "def best_feature(dataset):\n",
        "    features = dataset.columns[:-1]  # All columns except the target column\n",
        "    best_info_gain = -1\n",
        "    best_feature = None\n",
        "\n",
        "    # Calculate the information gain for each feature\n",
        "    for feature in features:\n",
        "        info_gain = information_gain(dataset, feature)\n",
        "        if info_gain > best_info_gain:\n",
        "            best_info_gain = info_gain\n",
        "            best_feature = feature\n",
        "\n",
        "    return best_feature\n",
        "\n",
        "# 4. Recursive function to build the decision tree (ID3 with simple pruning)\n",
        "def id3(dataset, max_depth=None, depth=0):\n",
        "    # Check for base cases\n",
        "    if len(dataset.iloc[:, -1].unique()) == 1:  # All examples have the same label\n",
        "        return dataset.iloc[0, -1]\n",
        "    if len(dataset.columns) == 1:  # No more features to split on\n",
        "        return dataset.iloc[:, -1].mode()[0]\n",
        "    if max_depth is not None and depth >= max_depth:  # Reached max depth\n",
        "        return dataset.iloc[:, -1].mode()[0]  # Return the most frequent class\n",
        "\n",
        "    # Choose the best feature to split on\n",
        "    best = best_feature(dataset)\n",
        "\n",
        "    # Create the tree as a dictionary\n",
        "    tree = {best: {}}\n",
        "\n",
        "    # Split the dataset based on the best feature and build subtrees\n",
        "    for value in dataset[best].unique():\n",
        "        subset = dataset[dataset[best] == value]\n",
        "        tree[best][value] = id3(subset.drop(columns=[best]), max_depth=max_depth, depth=depth+1)\n",
        "\n",
        "    return tree\n",
        "\n",
        "# 5. Example of pruning function (simple) - removes nodes that do not help improve accuracy\n",
        "def prune_tree(tree, test_data):\n",
        "    if isinstance(tree, dict):\n",
        "        for key, value in tree.items():\n",
        "            for sub_key, sub_value in value.items():\n",
        "                # Recursively prune the subtree\n",
        "                tree[key][sub_key] = prune_tree(sub_value, test_data)\n",
        "\n",
        "    # Here you would normally validate the tree and prune it based on test data\n",
        "    # For simplicity, let's just return the tree unchanged for now\n",
        "    return tree\n",
        "\n",
        "# 6. Example Dataset\n",
        "data = {\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'High', 'Low', 'Low', 'Low', 'High', 'Low'],\n",
        "    'Windy': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Weak', 'Weak', 'Strong', 'Weak', 'Strong'],\n",
        "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No']  # Target variable\n",
        "}\n",
        "\n",
        "# 7. Create the dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 8. Build the decision tree using ID3\n",
        "tree = id3(df, max_depth=3)\n",
        "print(\"Decision Tree:\", tree)\n",
        "\n",
        "# 9. Prune the decision tree (simple pruning, for now just returns the tree unchanged)\n",
        "pruned_tree = prune_tree(tree, df)\n",
        "print(\"Pruned Decision Tree:\", pruned_tree)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Helper function to calculate entropy\n",
        "def entropy(dataset):\n",
        "    # Count the occurrences of each class label in the dataset\n",
        "    class_counts = dataset.iloc[:, -1].value_counts()\n",
        "    # Calculate the entropy of the dataset using the formula\n",
        "    prob = class_counts / len(dataset)\n",
        "    return -np.sum(prob * np.log2(prob))\n",
        "\n",
        "# 2. Function to calculate information gain\n",
        "def information_gain(dataset, feature):\n",
        "    # Calculate the entropy of the original dataset\n",
        "    total_entropy = entropy(dataset)\n",
        "\n",
        "    # Get the values of the feature and their frequencies\n",
        "    feature_values = dataset[feature].value_counts()\n",
        "    weighted_entropy = 0\n",
        "\n",
        "    # For each feature value, calculate the weighted entropy of the split\n",
        "    for value, count in feature_values.items():\n",
        "        subset = dataset[dataset[feature] == value]\n",
        "        weighted_entropy += (count / len(dataset)) * entropy(subset)\n",
        "\n",
        "    # Return the information gain as the difference between the total entropy and the weighted entropy\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "# 3. Function to get the best feature to split on\n",
        "def best_feature(dataset):\n",
        "    features = dataset.columns[:-1]  # All columns except the target column\n",
        "    best_info_gain = -1\n",
        "    best_feature = None\n",
        "\n",
        "    # Calculate the information gain for each feature\n",
        "    for feature in features:\n",
        "        info_gain = information_gain(dataset, feature)\n",
        "        if info_gain > best_info_gain:\n",
        "            best_info_gain = info_gain\n",
        "            best_feature = feature\n",
        "\n",
        "    return best_feature\n",
        "\n",
        "# 4. Recursive function to build the decision tree (ID3 with simple pruning)\n",
        "def id3(dataset, max_depth=None, depth=0):\n",
        "    # Check for base cases\n",
        "    if len(dataset.iloc[:, -1].unique()) == 1:  # All examples have the same label\n",
        "        return dataset.iloc[0, -1]\n",
        "    if len(dataset.columns) == 1:  # No more features to split on\n",
        "        return dataset.iloc[:, -1].mode()[0]\n",
        "    if max_depth is not None and depth >= max_depth:  # Reached max depth\n",
        "        return dataset.iloc[:, -1].mode()[0]  # Return the most frequent class\n",
        "\n",
        "    # Choose the best feature to split on\n",
        "    best = best_feature(dataset)\n",
        "\n",
        "    # Create the tree as a dictionary\n",
        "    tree = {best: {}}\n",
        "\n",
        "    # Split the dataset based on the best feature and build subtrees\n",
        "    for value in dataset[best].unique():\n",
        "        subset = dataset[dataset[best] == value]\n",
        "        tree[best][value] = id3(subset.drop(columns=[best]), max_depth=max_depth, depth=depth+1)\n",
        "\n",
        "    return tree\n",
        "\n",
        "# 5. Function to print the decision tree in a tree-like format\n",
        "def print_tree(tree, depth=0):\n",
        "    # If the tree is a dictionary, it means it's a decision node\n",
        "    if isinstance(tree, dict):\n",
        "        for key, value in tree.items():\n",
        "            print(\"  \" * depth + str(key))  # Print the feature (node)\n",
        "            for sub_key, sub_value in value.items():\n",
        "                print(\"  \" * (depth + 1) + str(sub_key) + \":\")\n",
        "                print_tree(sub_value, depth + 2)  # Recur for the next level\n",
        "    else:\n",
        "        print(\"  \" * depth + \"-> \" + str(tree))  # If it's a leaf, print the class label\n",
        "\n",
        "# 6. Example Dataset\n",
        "data = {\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'High', 'Low', 'Low', 'Low', 'High', 'Low'],\n",
        "    'Windy': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Weak', 'Weak', 'Strong', 'Weak', 'Strong'],\n",
        "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No']  # Target variable\n",
        "}\n",
        "\n",
        "# 7. Create the dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 8. Build the decision tree using ID3\n",
        "tree = id3(df, max_depth=3)\n",
        "print(\"Decision Tree Structure:\")\n",
        "print_tree(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FcQ86WmC0mN",
        "outputId": "14c69bf6-c1df-41c6-bb86-0bd52a99c8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Structure:\n",
            "Outlook\n",
            "  Sunny:\n",
            "    Temperature\n",
            "      Hot:\n",
            "        -> No\n",
            "      Mild:\n",
            "        Windy\n",
            "          Weak:\n",
            "            -> No\n",
            "          Strong:\n",
            "            -> Yes\n",
            "  Overcast:\n",
            "    -> Yes\n",
            "  Rainy:\n",
            "    -> Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from graphviz import Digraph\n",
        "\n",
        "# 1. Helper function to calculate entropy\n",
        "def entropy(dataset):\n",
        "    class_counts = dataset.iloc[:, -1].value_counts()\n",
        "    prob = class_counts / len(dataset)\n",
        "    return -np.sum(prob * np.log2(prob))\n",
        "\n",
        "# 2. Function to calculate information gain\n",
        "def information_gain(dataset, feature):\n",
        "    total_entropy = entropy(dataset)\n",
        "    feature_values = dataset[feature].value_counts()\n",
        "    weighted_entropy = 0\n",
        "    for value, count in feature_values.items():\n",
        "        subset = dataset[dataset[feature] == value]\n",
        "        weighted_entropy += (count / len(dataset)) * entropy(subset)\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "# 3. Function to get the best feature to split on\n",
        "def best_feature(dataset):\n",
        "    features = dataset.columns[:-1]\n",
        "    best_info_gain = -1\n",
        "    best_feature = None\n",
        "    for feature in features:\n",
        "        info_gain = information_gain(dataset, feature)\n",
        "        if info_gain > best_info_gain:\n",
        "            best_info_gain = info_gain\n",
        "            best_feature = feature\n",
        "    return best_feature\n",
        "\n",
        "# 4. Recursive function to build the decision tree (ID3 with simple pruning)\n",
        "def id3(dataset, max_depth=None, depth=0):\n",
        "    if len(dataset.iloc[:, -1].unique()) == 1:\n",
        "        return dataset.iloc[0, -1]\n",
        "    if len(dataset.columns) == 1:\n",
        "        return dataset.iloc[:, -1].mode()[0]\n",
        "    if max_depth is not None and depth >= max_depth:\n",
        "        return dataset.iloc[:, -1].mode()[0]\n",
        "    best = best_feature(dataset)\n",
        "    tree = {best: {}}\n",
        "    for value in dataset[best].unique():\n",
        "        subset = dataset[dataset[best] == value]\n",
        "        tree[best][value] = id3(subset.drop(columns=[best]), max_depth=max_depth, depth=depth+1)\n",
        "    return tree\n",
        "\n",
        "# 5. Function to create a graphical decision tree\n",
        "def create_tree_diagram(tree, dot=None, parent_name=\"Root\", parent_value=\"\"):\n",
        "    if dot is None:\n",
        "        dot = Digraph(format=\"png\", engine=\"dot\")\n",
        "\n",
        "    # Recursively add nodes to the graph\n",
        "    if isinstance(tree, dict):\n",
        "        for feature, branches in tree.items():\n",
        "            feature_name = f\"{parent_name}_{feature}\"\n",
        "            dot.node(feature_name, feature)\n",
        "            dot.edge(parent_name, feature_name, label=parent_value)\n",
        "\n",
        "            for value, subtree in branches.items():\n",
        "                value_name = f\"{feature_name}_{value}\"\n",
        "                dot.node(value_name, f\"{feature}: {value}\")\n",
        "                dot.edge(feature_name, value_name, label=str(value))\n",
        "\n",
        "                # Recurse for each subtree\n",
        "                create_tree_diagram(subtree, dot, value_name, str(value))\n",
        "    else:\n",
        "        dot.node(parent_name + \"_class\", f\"Class: {tree}\")\n",
        "        dot.edge(parent_name, parent_name + \"_class\", label=\"Leaf\")\n",
        "\n",
        "    return dot\n",
        "\n",
        "# 6. Example Dataset\n",
        "data = {\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool','Mild', 'Cool','Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
        "    'Windy': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
        "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
        "}\n",
        "\n",
        "# 7. Create the dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 8. Build the decision tree using ID3\n",
        "tree = id3(df, max_depth=3)\n",
        "\n",
        "# 9. Create the decision tree diagram\n",
        "dot = create_tree_diagram(tree)\n",
        "\n",
        "# 10. Render and display the tree diagram\n",
        "dot.render(\"decision_tree\", view=True)  # This will generate a PNG file and open it in the default viewer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8Zhp0VJbHPJz",
        "outputId": "046ec97c-02b9-4883-ad06-869f9d144643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'decision_tree.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}